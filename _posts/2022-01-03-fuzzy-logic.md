---
title: "An Introduction to Fuzzy Pragmatist Bayesian Epistemology"
tags: ["epistemology", "pragmatism"]
---

## 1. Introduction

The goal of pragmatist epistemology is not to arrive at absolute truth, at certain knowledge about the universe, or any of the other goals which have occupied philosophers across the millennia. It has a more humble goal: to help human beings achieve our goals in a world that does not bend to our every whim, a world that is more than happy to slap us in the face with our misconceptions and preconceived notions at every turn. It offers a way to adapt to the world, to move with the current instead of against it, by providing a toolkit for predicting future experiences — the outcomes of our actions, the changes in the world around us — based on the only information we have available to us: past experiences. Human beings are thrust into a world that effects us on a deep and intimate level, one which we can only close our eyes to by committing suicide; therefore we have a choice: death, or learning. Choose to shut the world out permanently, or learn how to operate within it. We cannot know whether we are a brain in a vat, or being deceived by a demon, or NPCs in a cosmic simulation run by higher beings. But until someone offers us the red pill — a way to leave the simulation, if it is one — this world is as real to us as it gets, as real as any other. We have no choice but to face it, to live in it, and to try to live in it well.

There are therefore three core insights which form the foundations of pragmatist epistemology. First, that I experience things, and these experiences effect me. Second, that something which is not a product of my desires or beliefs, but which can influence these things, is what produces my experiences. Third, that I only have my connection, through my experiences past and present, to this external thing. The implications of these axioms form my epistemology, and in this essay I will lay out these implications.

## 2. Definitions

To begin with, we must lay out some definitions. First among them is the definition of "truth." What, in the pragmatist framework, does it mean for a proposition to be true? Considering that the goal of pragmatist epistemology is to arrive at propositions that help us navigate our world, it seems clear that the goal should be propositions that answer questions about future experiences, since these kinds of questions — what will happen when I push this? what will I see when I turn the next corner? when will winter arrive? — are the most helpful in understanding how to live and act. If the goal is predictive propositions, then the fullest satisfaction of that goal would be propositions that are *perfectly* predictive — i.e. propositions that predict the next experiences of any agent under any circumstance relevant to the subject of the proposition, past, present, future, in any place, for anyone, at any time. Since the definition of truth is relative to the goal of epistemic framework one is operating within, the definition of truth in my framework is just such a perfectly predictive proposition. Thus, a proposition is true if it predicts the experiences of any agent, at any time, in any place, regarding the subject of the proposition.

To be truth-apt, therefore, a proposition must be one that can be used to predict experiences with regard to the subject of said proposition. Otherwise, within this framework, there is not really any sense in which the proposition can be counted to be true to some degree, or false to some degree. It is important to note that since the range over which a proposition's predictiveness can vary, and the accuracy of its predictions can also vary, truth in this framework is a spectrum. The ultimate, ideal goal, as described above, is 100% predictiveness in both detail and scope; propositions can actually be closer to or farther away from this goal as their accuracy and scope differ. Truth is a spectrum, and it varies across several variables. 

Since the truth of a proposition depends on whether it accurately predicts the experiences that agents could have with regards to its subject, meaningful  propositions must always ultimately be about experiences of some kind, about *interactions* with an object. The meaningfulness of any proposition is dependent on its capacity to effect the experiences of some agent. To clarify, it is not relative to its actual effect on any existing agent, because that would mean statements about things that no one can yet check, but could conceivably be checked in the future, would be meaningless; instead the truth of a proposition is relative to its possible effects on any *conceivable* agent: thus if someone *could* check concerning whether a proposition accurately describes a certain experience or not, then the proposition is meaningful. Conversely, if no one could *possibly* check whether a proposition accurately describes some experience concerning its subject, then the proposition is meaningless, useless, and empty nonsense. Note that meaningfulness is not identical with truth-aptness. This is because some statements are definitions and other *a priori* true statements, which simply assign names to certain experiences or derive conceptual conclusions from them. They need to be about experience in some sense in order to be meaningful because without that then they couldn't really be used to come to true or false conclusions, wouldn't really be "about" anything that is really relevant to pragmatism and human life, but they don't need to make statements *about* some experience, that's just a requirement for truth-aptness. 

There is such a thing as "referred" meaning or truth-aptness, however. A proposition that is not directly about an experience can be meaningful as it relates to some conceptual apparatus that ultimately says something about experience, even though without it it would be pointless and useless. Likewise, a proposition could be true to the degree that the system of propositions (that it is derived from or that are in part derived from it) can lead to true propositions in the final conclusion. An example of this last type of thing is clearly to be found in physics. In physics, the givens of a problem are "true," in that they describe experiences of a thing, and the results of the physics problem are also (hopefully) true, in that they predict future experiences based on those givens. But the intermediate steps of the mathematics aren't really "true" in any real sense, apart from what they imply about the starting point and ending point. Oftentimes, if you try to translate what the intermediate algebra steps in a physics problem "mean" by applying the standard definitions of the variables, as you use them at the beginning and end, you'll get nonsensical and often clearly counterfactual claims about reality. This is why when people doing theoretical physics try to derive claims about how reality works from the mathematics they are using to describe that reality, one should accord their claims only a portion of the credence you accord to their predictions overall. It might be the case that their theories are predictive because, when investigated, the intermediate steps of the mathematical process are propositions that accurately describe how the intermediate steps of the physical process look when we check, but it could almost equally be the case that this is not so.

At this point one might be worrying that this is verging too closely to the picture theory of language put forward by Wittgenstein in the *Tractatus*, and that therefore these definitions might fall afoul of the same problem that his did, in rendering the very philosophical text that put them forward ultimately meaningless. This is not the case, however. I have been very careful to state that propositions are about experiences, but not that these experiences necessarily must be about the outside world external to our consciousness. Therefore, psychological propositions and others that speak about our own internal experiences of existence are just as truth-apt as ones that speak about our experiences of the world, although they are much more difficult to demonstrate to others. Thus when I talk about how to use pragmatist epistemology later in this essay, and discuss how to arrive at propositions that are more true or less false, this is still a statement about experience, because I am describing how some methods will produce the experience of expected outcomes more often than the experience of unexpected ones. And the definitions I gave above are derived from propositions about experience from the first section, and will influence propositions about experience later, and are, furthermore, in themselves, propositions about experience in a sense, as the definition of truth for instance has itself to do with experiences of expected or unexpected outcomes. Although they don't *predict* things and are therefore not truth-apt, definitions are still about experiences, in that they put labels on them, and so they are still meaningful.

In the next sections, I will defend the individual elements of my analysis. Once that is complete, I will assemble them into a form of fuzzy Bayesian induction, and a procedure for setting the priors of that Bayesian form.

## 3. Why Induction

When operating in the world, we are forced to take action whether we are certain in our beliefs about the world or not — the entire condition of our existence is that of being forced to act on the basis of what we guess or think about the world, to deal with uncertainty and to have the courage to act despite it. Induction is, in essence, merely a reification of that principle: although no reasoning can directly support or substantiate induction without first assuming it (as Hume showed), since we have no other information about how the world will effect us, and what experiences it will present to us, than the experiences it has given us in the past, our best bet is to attempt to derive the best picture we can of the world based on those past experiences. Fortunately, induction is a self-correcting process: if our expectations, based on past experiences are wrong, and the world actually appears to us in a different way than we were led to expect, then we need only either correct our expectations, by either learning how these contrasting experiences can be subsumed under the same principle, thereby granting the weight of *all* of our past experiences to a new guess, or come up with a new principle and reinterpret our older experiences.

Furthermore, there is no real reason to expect that such a process wouldn't work. After all, there is no *prima facie* reason to assume that the default state of the world is endless chaotic flux, always changing, perhaps in need of some lawgiver to subjugate it, instead of assuming the opposite, that the inherent nature of the world is that of a clockwork machine that proceeds according to orderly laws. Actually, there is no possible benefit to navigating the world at all to assume the former, whereas there is a chance of benefit in assuming the latter. For, if I choose to believe that the universe is chaotic, then I will constantly be confused and adrift whether it is or it isn't because I will have nothing to go on, not even my past experiences, and therefore no ability to even *guess* what happens next; whereas if I choose to believe that it is orderly, then I will be constantly foiled if it is chaotic, but able to act sensibly if it is not. This is the core reason that everyone, even those who deny induction, ultimately cannot help but act as if induction is true: they have to act, and acting on the basis of nothing is guaranteed to only have a random success rate, whereas acting on the basis of past experience at least conceptually has some chance of having a higher-than-random success rate. One can represent these choices with a payoff matrix:

|               | Universe: Chaos | Universe: Order |
| ------------- | :-------------- | :-------------- |
| Assume: Chaos | 0               | 0               |
| Assume: Order | 0               | 1               |

On top of that, there is another thing to consider: all else being equal, the default assumption should be that nothing has changed. That should be the null hypothesis, as that is the simplest assumption to make: when introduced to something, nothing needs to happen, no new concept or element needs to be added, for it to stay the same. Meanwhile, for something to change, that introduces two entities: the thing before the change, and the thing after. There can be reasons to reverse this assumption, as to assert it absolutely would be to bring a Principle of Sufficient Reason into play without due justification, but again changing and staying the same are at least equally likely, and one of them is a bit less complex. This, however, should be dealt with more in the next section, on the principle of parsimony.

## 4. Why The Principle of Parsimony

The principle of parsimony, more commonly referred to as Occam's Razor, might, at first blush, seem to be a rather arbitrary statement of theoretical preference. Why *not* multiply the entities involved in a theory beyond what is strictly necessary? After all, there is no law of the universe saying that everything has to be as simple as possible to produce the experiences necessary. If we look deeper, however, it quickly becomes clear that the principle of parsimony is a deeply integral part of any consistent and rational attempt to build theories about anything.

The first factor that makes the principle of parsimony necessary is a probabilistic one. Because there is no such thing as certain knowledge outside one's own head, any proposition not about incorrigible phenomenological experiences has a non-zero chance of turning out to be wrong. Therefore, the more propositions are required to describe a theory, the greater the probability that the theory will turn out to be incorrect.  Therefore, *ceterus paribus*, we should favor theories that require few propositions to describe, in order to minimize our chances of being wrong. This factor can be overcome, however, if the propositions you are using in your simpler theory are more probably wrong than the propositions you use in a more complex one, such as would be the case if they either were worse descriptions of experience, made worse predictions, or led to worse predictions (through referred truth). It is important to clarify, here, that when I'm talking about probabilities, I'm using them in a compound sense to refer to a combination of Bayesian credence-probability, accuracy of description, and percentage of possible worlds, *not* in the sense of frequentist probability that relies on past experiences. Therefore, the probability of a proposition being incorrect relies on the possibility that there are other worlds that would produce the same experiences that led be to construct that proposition, for which that proposition is not true, the accuracy of how well that proposition describes the experiences it is supposed to be about, and how well it fits with other propositions known to me.

The second factor that makes the principle of parsimony a good starting point for theory-building is that it is a natural starting point at the limit. In other words, when starting to build a theory about some phenomena, you can either start at the simplest end of the spectrum, a theory composed of a single proposition, or you can start at the other end of the spectrum with an infinitely complex theory composed of an unlimited number of propositions. The one end of the pole is represented by the principle of parsimony, the other the logical conclusion if you believe that theories should be arbitrarily complex. If you start at the simpler end, then you can build up complexity as needed, as prediction failures require you to increase the complexity of the theory, while if you start at the other end, there can be no justification for decreasing the number of propositions involved in your theory, besides the pragmatic one that it's too hard to do and so fewer propositions are needed — but that's an argument for the principle of parsimony too, since if complexity making theories difficult to conceptualize or use is a factor in the practicality of a theory, then trying to make a theory as simple as possible while keeping it functional is the natural conclusion.

The third factor is that the *entire point* of developing a theory in the first place is to simplify: to find an underlying principle behind many diverse propositions, in order to make the world navigable, instead of treating each experience as a totally separate and unique event which can be filed away as a proposition but not linked to anything else. Thus, inherent in the very concept of pragmatic epistemology, and epistemology in general for that matter, is simplification: finding the principles that underlie things. If you're going to deny the principle of parsimony, for consistency's sake you have to give up knowing or guessing anything whatsoever.

Another important implication of the principle of parsimony is that a theory should be parsimonious not just in its description, but in its implications. A theory that has a lot of implications — that makes a lot of predictions, in other words — that have not yet been verified and confirmed to be in accordance with it, is a theory that is very likely to make a prediction wrong, since each prediction has a chance of being wrong because some detail of the theory matches many areas in past and future experiences but not all. Therefore, theories with wide-ranging implications are to be preferred only if those implications can be, and are, verified to be accurate — in which case those implications add to the likelihood of the theory, since the more of past experiences it explains the more future experiences it is likely to.

There are many subsidiary practical reasons to favor a simpler theory, as long as it is capable of explaining the same data, over a more complex one. For one thing, a more complex theory is more likely to be contradictory, to have hidden assumptions, and to have logical errors. The larger a theory grows, the greater these chances, get, so a theory that is more complex than it absolutely needs to be should be looked at with suspicion. In a similar vein, a larger and more complex theory is more unwieldy, meaning it is more difficult to generate predictions, and even more difficult to make sure the predictions one gets from it actually follow from it. Since I am pursuing a pragmatic theory of epistemology, these considerations should certainly count for something!

There are other ways to measure the complexity of a theory than counting the number of propositions required to describe it. These ways will ultimately *reduce* to that, which is why they are pretty much equivalent in justification, but depending on the circumstances they may be a lot easier to actually check than breaking down your hypothesis into symbolic logic and then into basic propositions (where conjugated propositions count as multiple). I'll make a table, with each entry in order of relative weight in one's consideration. The distance between the weights is exponential of some variety, but the specific distance between the weights will change depending on the situation.

| **Short Name**                   | **Description**                                              | **Justification**                                            |
| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Entity type number (*ETN*)       | The number of types of entities                              | It often requires a huge amount of statements about things to introduce a new type of entity |
| Type complexity (*TC*)           | The complexity of the types of entities required             | This is merely an expression of a feature the former         |
| Phenomenological distance (*PD*) | The distance between the appearance of how things work, and how they are supposed to work in the theory | Appearances are all we have to work with by default, so introducing more distance between experience and theory introduces room for error as the two diverge |
| Entity number (*EN*)             | The number of entities                                       | It requires more statements about the world to add new entities |

Since all of these have to do with different components of the overall propositional complexity, then you can either sum them up or multiply them (as the case requires for the individual aspect) without fear of double-counting. IF there are any other aspects of the complexity of a theory that are left out, you can also add those in. In essence, then, you can look at the complexity of a theory as a mathematical equation, like this:

![img](https://tbdp-web.github.io/blog/assets/math1.png)

Then just add in any variables or values you should to capture other parts of the propositional complexity not dealt with by the variables in the table above.

Be careful though, as I will explain later, the mathematics to be found in this essay are *strictly* for clarifying my words. It is impossible to come up with non-arbitrary numeric values for most of the variables I'm talking about anyway, so the attempt to do so, in order to actually do the algebra, would just multiply and reproduce any mistakes you've made, and give a false sense of precision and accuracy. It is better to treat each variable as a vague quantity, or a value with substantial error bounds.

## 5. Kinds of Predictions

If we are assessing the success or truth value of a proposition on the basis of how well it predicts experiences, it is necessary to have a clear idea of what exactly is meant by prediction. The prototypical case of prediction, the one that is primarily used in science, is the *ex ante* prediction. To put it in experiential/pragmatic terms, an *ex ante* prediction is a proposition about a certain experience of its subject which describes certain details of that experience before that experience takes place, typically along with some indication of how that experience might be generated, how it can be observed, or when it will occur. For instance, "if you touch the stove while it is on, you will get burned." An accurate prediction is one which, to borrow from William James for a moment, creates an experience of recognition in a person who is first given the prediction and then exposed to the subject of said prediction. If one is not surprised by the experiences which one's actions or existence relative to a subject generates (surprise being the opposite of expectation/recognition), than this implies that any reasoning or planning one made on the basis of one's expectations will also be confirmed, which is what allows one to act and plan, since if one expects certain outcomes if one thing happens, and other outcomes if another does, then one can act towards making one or the other of the outcomes happen in order to get positive experiences, and if this leads to expected outcomes, then that means the planning worked, and if this is consistently possible then planning is possible.

Another type of prediction is *ex post facto* prediction. This is a much more tricky kind of prediction, because it works by deriving your theories from experiences that are already available, which means that it is all too easy to create a *ad hoc* theory. An *ad hoc* theory is one that is randomly thrown together with no actual predictive power beyond the data that was given for its construction, because it presents either no underlying principle, or an underlying principle that is actually inaccurate and then jam-packed with exceptions in order to fit past experiences, indicating the need for more exceptions in the future and therefore obviating its predictive ability. A stronger attention to the principle of parsimony, and careful attention to avoiding making exceptions or stretching the truth in order to make a theory fit, is needed for *ex post facto* predictions to actually confirm a theory.

One of the most common mistakes made when *ex post facto* predictions (predictions of experiences already available to the one constructing the theory) are used to evaluate a theory is to confuse a theory being *compatible* with some experience occurring with a theory that *predicts* some experience. A theory needs to not just be able to be made to fit with some experience, it actually needs to predict that experience as being more likely than any relevantly different experience — only if that is the case is the theory actually any good for producing *ex ante* predictions, which are the kind that are actually useful. A theory that predicts all experiences equally, with a non-zero probability for each, or a too-wide range of experiences with equal or nearly indistinguishable probability, is a theory that actually provides no useful information about how to set our expectations *ex ante*, and therefore no useful information about how to act or live, while it can be fit with nearly any experience after the fact because it "predicts" nearly any experience. For instance, pretend someone handed you a black box with a button on the side and a screen on top that displayed a single digit number. Which "theory" about how the box works would be the most useful to you: one that said that the box had an 80% chance of showing a 1, a 10% chance of showing a 2, and a 10% chance of showing anything else, or a theory that said that it could show the digits 0-9? Assuming it is correct, the first theory gives you far more actionable information, while the second theory tells you very little. However, the first theory is more likely to be wrong, in that it would be more difficult to reconcile it with some experiences over time with the box, while the second theory could pretty much be reconciled with any experience of the box, but it tells you basically nothing. Thus for an *ex post facto* prediction to actually *be* a prediction of a theory, that theory has to actually be able to make strong predictions. The weaker the differentiation between all its outcomes, and the more predictions a theory makes about the same event's outcome, the weaker it "fitting" with an event is as evidence for it. This is why omnipotent, deities, for instance, are really not great as explanatory theories: they don't really predict any outcome any as more likely or possible than any other, and are compatible with any outcome.

## 6. Putting it All Together

So, we have two components: the principle of parsimony, and the principle of induction. The principle of induction indicates the likelihood of a theory being predictive ("true") given a list of past experiences, by saying that the probability of a theory being true is related to the confidence with which that theory predicted past experiences that actually happened. If the theory predicted something that happened confidently, that should increase one's credence in it,  in proportion to the confidence; if it predicted something that did not happen, one's credence should decrease in accordance; and if the theory didn't predict something that happened to the subject which the theory was about, that should be counted as it having predicted with complete confidence that that thing wouldn't happen, which should be a fairly serious blow to it. The principle of parsimony indicates the inherent improbability of a given theory, the hurtle that must be *overcome* by the theory's predictive power in order for one to lend credence to a theory — where, as the predictive power of the theory grows over and above the inherent improbability, one's credence rises as well. Importantly, then, the *default* position should be not lending any credence to any particular theory at all. This is because to make any positive statement about something is to risk being wrong, and therefore to set one's expectations wrong, which can lead to committing to courses of action that will lead to disappointment or even harm. This is also because accepting one theory before it has risen to its "burden of proof" (overcome its inherent improbability) would require, for consistency, one to accept all theories — all infinitely many of them! This of course would be both mentally impossible, and lead to contradictions, as well as being only slightly better than accepting random theories as far as setting expectations goes.

To formalize all that I said in the previous (large) paragraph, we can adopt Bayes' Theorem:

![Bayes' theorem](https://wikimedia.org/api/rest_v1/media/math/render/svg/87c061fe1c7430a5201eef3fa50f9d00eac78810)

Here, *P(A \| B)* refers to the probability that the theory (A) is true, given the fact that some experience (B) was had — i.e. the credence one should have in A after B happens. Likewise, *P(A)* refers to the prior probability of the theory, A. Before any evidence is added to the mix to get induction working to confirm or deny A, this is equal to the negation of the complexity of A, since the probability of A starts out smaller in proportion to its inherent improbability. *P(B)* is the probability of the experience happening at all. And *P(B \| A)* is the probability (the confidence) with which the theory predicts the experience that is being added to the pile of data.

This theorem is applied recursively: for each no experience, the *posterior* probability of A is calculated given that that experience happened, and then that becomes the *prior* probability of A for the next time we get a new experience relevant to the theory, since it the posterior probability represents the new level of credence we have in A after the experience B.

I should actually clarify something I said above: the prior probability of a theory is not *literally* 100% minus the complexity of a theory. That would imply all kinds of strange things, especially since the default needs to be *not* believing in a theory and that would imply that even before any evidence, if a theory is simple enough we should have a high credence in it. Instead, credence should start out at 50% — essentially agnostic to the truth or falsity of a theory, and then the complexity of a theory should (through some log function probably, to avoid an absolutely zero starting credence for a non-infinitely complex theory) subtract from that starting credence value. So something like this:

![img](https://tbdp-web.github.io/blog/assets/math2.png)

Then, in order to have the probabilities weigh against each other proportionately, instead of having the complexity weigh less than it should, each adjustment of the evidence in favor of a theory should also be decreased by the corresponding (reversal of) the logarithm of the complexity of the theory. That way the inherent improbability of a theory is weighed equally against the probability benefits that evidence brings to it, but this does not lead to a zero or negative prior probability, or to positive evidence weighing against the theory. To sum this up in a verbal way that doesn't require mathematics: you should never start more than agnostic to a theory, either in the positive or negative direction, but your starting places should be heavily informed by how complex the theory is to start with; and positive evidence should always weigh in a theory's favor, but it should weigh much less in favor of a more complex theory than a simpler one that also predicted that evidence.

## 7. What Are Mathematics and Logic?

Since I've been using so much mathematics in this theory, now is probably a good time to talk about what I think mathematics and logic even are, and how they relate to the rest of my epistemology. It should be noted here that my justification of the accuracy of mathematics or logic should not lend to, or take away from, your credence in the foregoing, since there mathematics is not used as a justification whatsoever, but merely a formalization and clarification of what I have clearly explained in logical terms.

Logic is an attempt to formalize what "makes sense to us" conceptually, and how we experience the basic relations between experiences. It is a theory like any other, derived from experience, but it is different from others in one crucial fact: it is derived in part from our experiences of our own minds and how they operate. That is why the three basic laws of logic are often called the three "laws of thought." Therefore they are more certain than most theories are, and additionally they are impossible to fully discard, since they describe indelible experiences of how our minds work that we can't alienate ourselves from without alienating thought itself. The fact that logic is constructed can be seen in the fact that there are many systems of logic, each with different ways of explaining how everything works and what "makes sense."

Mathematics is somewhat similar. Mathematics is a symbolic language that we have created in order to be able to more succinctly describe certain classes of problems and relations. We have introduced axioms, theorems, and relations between the symbols of this language both as definitions (to limit the class of things that symbols can talk about to make them clearer and more precise), and as attempts to describe certain aspects of our reality or certain implications of these basic axioms and rules. The reason mathematics seems to have such a stunning ability to match up with our experiences of the external world is precisely because it is a very flexible language, where we only apply its tools when they fit to the real situation, and where we change or invent new tools when the situation changes or when we encounter new situations. The reason it can often feel like discovery, even still, is because much of mathematics is about tracing out the implications of its axioms and rules, and the reason it can feel like an art sometimes is that it is partially derived from our intuitions about how things work. Likewise, the reason you don't always have to check with experiences of reality while doing mathematics is that when one is doing mathematics most of the time one is simply creating new tools — finding out which real situations they actually match onto is something that will be dealt with by others, after the fact. For instance, someone invented the parabola by playing around with the rules of the system, it was only afterward that someone looked around for things to match the parabola too. Thus being surprised that mathematics matches to the world is being surprised that a flexible language, which can describe as many unreal things as real, can have a subset of it used to describe something when you look around and carefully choose which of its tools to use to describe it. It's like being surprised that you can use words to describe a rainbow, or that someone could sit down and write a description of it. 

## 8. Conclusion

So, pragmatically speaking, an epistemology that is best designed to manage the inherent complexity of the world, and the inherent need of humanity to act in spite of it, is a form of empiricist Bayesianism. Notice that, although much of what I said about prediction and so on might *sound* scientistic to a fault, mainstream science doesn't actually use Bayesian methods, only poor and somewhat arbitrary approximations of them, and it still uses the falsification criteria, which is a poor imitation of my criteria for something to be truth-apt.
